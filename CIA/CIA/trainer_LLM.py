import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Optional
from models_LLM import SelfSupervisedModel
from data_processor import GraphDataProcessor, analyze_graph_statistics
import os
from utils import _client
import re
from tqdm import tqdm
import numpy as np
from sklearn.metrics import roc_curve, auc
from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import datetime
import logging
import argparse
import json

EDGES_SYSTEM_GLOBAL_MI = """
Role: You are an expert in Multi-Agent System (LLM-MAS) architecture analysis. 
Your core capability is Topology Inference: infer the hidden communication graph by analyzing textual interaction logs.

Task: Analyze the provided agent interaction records to identify contextual dependencies between agents. You must quantify the likelihood of a directed edge existing from Agent $i$ to Agent $j$ (Edge $[i \to j]$) using a Confidence Score (0-100%).
Confidence Scoring Framework:Assign a score based on the strength of the evidence found in Agent $j$'s response relative to Agent $i$'s output:

High Confidence (80-100%): Explicit Reference & Strict Execution.
Explicit Citation: Agent $j$ explicitly mentions Agent $i$ (e.g., "Using the code provided above...", Agent $j$ explicitly acknowledges and reviews the response from Agent $i$.).
Hard Dependency: Agent $j$ executes code, runs a SQL query, or parses a specific data structure that was strictly defined or generated by Agent $i$. Agent $j$ cannot function without this specific asset.

Medium Confidence (50-79%): Content Dependency & Semantic Alignment.
Unique Information Flow: Agent $j$ solves a problem using unique information or a specific plan provided only by Agent $i$, even without explicit citation.
High Semantic Similarity: Agent $j$'s content exhibits high semantic overlap with Agent $i$. This includes reusing specific terminologies, variable names, or continuing a niche topic introduced by $i$, indicating a direct continuation of the context.
Logical Mapping: There is a clear "Question $\to$ Answer" or "Task $\to$ Solution" mapping between $i$ and $j$.

Low Confidence (<50%): Weak Logical Sequence.
Agent $j$ and Agent $i$ have a weak logical sequence, the content is generic, or the solution could have been derived independently without Agent $i$'s specific input.

Execution Steps:
1. Scan: Read the logs and identify all potential sender-receiver pairs based on the conversation flow.
2. Evaluate: For each pair, apply the Confidence Scoring Framework. specifically checking for semantic overlap and unique keywords.
3. Rank: Sort all identified edges by their Confidence Score in descending order.


1.  **Edge Direction**: An edge `[i, j]` means information flows from **i to j** (i.e., j depends on i).
2.  **ID Consistency**: Use the exact Agent IDs (e.g., 0, 1, 2) provided in the input.
3. Completeness: Output the Top 3 Highest Confidence Edges.
4. No Markdown: Do not use markdown code blocks.
5. List Length: The output JSON list must contain exactly one element.

Provide strictly a raw JSON list in the following format. Do not include any introduction or conclusion text.

[{{"edge": [source_id, target_id], "reason": "Brief explanation of the dependency"}},{{"edge": [source_id, target_id], "reason": "Brief explanation of the dependency"}},{{"edge": [source_id, target_id], "reason": "Brief explanation of the dependency"}}]
"""

EDGES_USER_TPL_GLOBAL_MI = """
Here is the data for the current analysis session.
**[Agent Output Records]**
{nodes_block}
Based on the criteria defined in the system instructions, please Output the Top 3 Highest Confidence Edges in raw JSON format.

[{{"edge": [source_id, target_id], "reason": "Brief explanation of the dependency"}},{{"edge": [source_id, target_id], "reason": "Brief explanation of the dependency"}},{{"edge": [source_id, target_id], "reason": "Brief explanation of the dependency"}}]
"""



class SelfSupervisedTrainer:
	def __init__(self,
			 data_path: str,
			 outputs: str,
			 encoder: str,
			 text_encoder_name: str = "/data/llm/all-MiniLM-L6-v2",
			 device: Optional[str] = None,
			 hidden_dim: int = 256,
			 out_dim: int = 128,
			 num_gnn_layers: int = 2,
			 dropout: float = 0.1,
			 tokenizer_max_length: int = 128,
			 mi_temperature: float = 1.0,
			 lr: float = 2e-4,
			 weight_decay: float = 1e-4,
			 alpha: float = 1.0,
			 beta: float = 1.0,
			 edge_threshold: float = 0.5,
			 model_name: str = "NWJ",
			 structure: str = "Linear",
			 total_steps: int = 100,
			 step: int = 0,
			 max_agents: int = 5,
			):
		self.device = device
		self.edge_threshold = edge_threshold
		self.alpha = alpha
		self.beta = beta
		self.total_steps = total_steps
		self.step = step
		self.initial_lr = lr
		self.outputs = outputs
		self.model = SelfSupervisedModel(
			text_encoder_name=text_encoder_name,
			device=self.device,
			hidden_dim=hidden_dim,
			out_dim=out_dim,
			num_gnn_layers=num_gnn_layers,
			dropout=dropout,
			tokenizer_max_length=tokenizer_max_length,
			mi_temperature=mi_temperature,
			data_path=data_path,
			encoder=encoder,
			model_name=model_name,
			structure=structure,
			max_agents = max_agents,
		)
		self.model = self.model.to(self.device)
		self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)
		self.processor = GraphDataProcessor()


	def _get_current_lr(self) -> float:
		warmup_end = int(0.2 * self.total_steps)
		full_end = int(0.5 * self.total_steps)
		
		if self.step < warmup_end:
			current_lr = self.initial_lr
		elif self.step < full_end:
			progress = (self.step - warmup_end) / max(1, (full_end - warmup_end))
			current_lr = self.initial_lr * 0.1 
		else:
			current_lr = self.initial_lr * 0.1
		
		return current_lr
		
	def _calc_accuracy(self, pred_adj: torch.Tensor, true_adj: torch.Tensor) -> float: 
		with torch.no_grad():
			correct = (pred_adj == true_adj).float().mean().item()
		return correct

	def _calc_precision(self, pred_adj: torch.Tensor, true_adj: torch.Tensor) -> float:
		with torch.no_grad():
			pred_positive = (pred_adj == 1)
			true_positive_count = ((pred_adj == 1) & (true_adj == 1)).sum().item()
			pred_positive_count = pred_positive.sum().item()
			if pred_positive_count == 0:
				return 0.0
			return true_positive_count / float(pred_positive_count)

	def _calc_auc(self, score_adj: torch.Tensor, true_adj: torch.Tensor) -> float:

		with torch.no_grad():
			assert score_adj.shape == true_adj.shape
			device = score_adj.device
			n = score_adj.size(0)
			mask = ~torch.eye(n, dtype=torch.bool, device=device)
			scores = score_adj[mask].reshape(-1)
			labels = true_adj[mask].reshape(-1).float()
			n_pos = labels.sum()
			n_neg = labels.numel() - n_pos
			if n_pos == 0 or n_neg == 0:
				return float('nan')
			order = torch.argsort(scores, descending=True)
			labels_sorted = labels[order]
			pos = (labels_sorted == 1).float()
			neg = (labels_sorted == 0).float()
			cum_tp = pos.cumsum(dim=0)
			cum_fp = neg.cumsum(dim=0)
			tpr = cum_tp / (n_pos + 1e-8)
			fpr = cum_fp / (n_neg + 1e-8)
			fpr = torch.cat([torch.zeros(1, device=device), fpr])
			tpr = torch.cat([torch.zeros(1, device=device), tpr])
			dfpr = fpr[1:] - fpr[:-1]
			auc = ((tpr[1:] + tpr[:-1]) * 0.5 * dfpr).sum().item()
			return float(auc)

	def _calc_recall(self, pred_adj: torch.Tensor, true_adj: torch.Tensor) -> float:
		with torch.no_grad():
			true_positive_count = ((pred_adj == 1) & (true_adj == 1)).sum().item()
			actual_positive_count = (true_adj == 1).sum().item()
			if actual_positive_count == 0:
				return 0.0
			return true_positive_count / float(actual_positive_count)


	def _calc_f1(self, pred_adj: torch.Tensor, true_adj: torch.Tensor) -> float:
		with torch.no_grad():
			precision = self._calc_precision(pred_adj, true_adj)
			recall = self._calc_recall(pred_adj, true_adj)
			if precision + recall == 0:
				return 0.0
			return 2 * (precision * recall) / (precision + recall)




	def _build_global_edges_messages_llm(self,
                                         node_ids: list,
                                         node_outputs: list,
                                         ):
		nodes_block = "\n".join([f"- {nid}: {node_outputs[i][0]!r}" for i, nid in enumerate(node_ids)])
		return [
			{"role": "system", "content": EDGES_SYSTEM_GLOBAL_MI},
			{"role": "user", "content": EDGES_USER_TPL_GLOBAL_MI.format(nodes_block=nodes_block)},
		]
	def _safe_json_loads_for_edges(self, s: str):
		s = (s or "").strip()
		import json, re
		try:
			return json.loads(s)
		except Exception:
			m = re.search(r"\{[\s\S]*\}", s)
			if m:
				try:
					return json.loads(m.group(0))
				except Exception:
					return {}
			return {}


	@torch.no_grad()
	def llm_predict_edges_global_llm(self,
                                id_list: list,
                                node_texts: list[str],
                                device: str | None = None):


		messages_edges = self._build_global_edges_messages_llm(
			node_ids=id_list,
			node_outputs=node_texts,
		)
		obj={}
		trys=0
		while trys<5 and obj=={}:
			raw_edges = _client.chat.completions.create(
                        model="gpt_5",
                        messages=messages_edges,
                    )
			trys+=1
			obj = self._safe_json_loads_for_edges(raw_edges)
		pair_log = []

		if isinstance(obj, dict) and "edge" in obj:
			obj = [obj]
		if isinstance(obj, list):
			for edge in obj:
				if isinstance(edge, dict) and "edge" in edge:
					pair_log.append(edge["edge"])
				elif isinstance(edge, (list, tuple)) and len(edge) == 2:
					pair_log.append(edge)
		return pair_log



	def train_epoch(self,g):
		self.model.train()
		avg_loss = 0.0
		avg_rec_loss = 0.0
		avg_s_loss = 0.0
		avg_align_loss = 0.0
		avg_sup_loss = 0.0
		accs: List[float] = []
		f1s: List[float] = []
		aucs: List[float] = []

		sim_accum = None
		node_outputs_all: List[List[str]] = self.outputs
		true_adj = g['adjacency_matrix'].to(self.device)
		first_perturb_group = node_outputs_all[0]
		pair_log = self.llm_predict_edges_global_llm(
				id_list=[str(i) for i in range(len(first_perturb_group))], 
				node_texts=first_perturb_group,
				device=str(self.device),
			)
		
		num_nodes = len(node_outputs_all[0])
		pair_log_adj = torch.zeros((num_nodes, num_nodes), device=self.device)
		for edge in pair_log:
			if len(edge) == 2:
				source_id, target_id = int(edge[0]), int(edge[1])
				if 0 <= source_id < num_nodes and 0 <= target_id < num_nodes:
					pair_log_adj[source_id, target_id] = 1.0
		self.optimizer.zero_grad()
		# 将 node_outputs_all 从按 perturb 组织转换为按 agent 组织
		node_outputs = [list(x) for x in zip(*node_outputs_all)]
		task=g['task']
		outputs = self.model(node_outputs, task,pair_log)
		s_loss = outputs['s_loss']
		rec_loss = outputs['rec_loss']
		align_loss = outputs['align_loss']
		sup_loss = outputs['sup_loss']
		sim = outputs['sim']
		if sim_accum is None:
			sim_accum = sim.clone()
		else:
			sim_accum += sim

		loss = SelfSupervisedModel.loss_self_supervised(
			s_loss, rec_loss, align_loss, sup_loss, sim, self.step, self.total_steps,
			alpha=self.alpha, beta=self.beta
		)
		num_groups=1
		loss.backward()
		avg_loss += loss.item()
		avg_rec_loss += rec_loss.item()
		avg_s_loss += s_loss.item()
		avg_sup_loss += sup_loss.item()
		avg_align_loss += align_loss.item()
		self.step += 1

		current_lr = self._get_current_lr()
		for param_group in self.optimizer.param_groups:
			param_group['lr'] = current_lr
		self.optimizer.step()
		avg_loss = avg_loss / max(1, num_groups)
		avg_rec_loss = avg_rec_loss / max(1, num_groups)
		avg_s_loss = avg_s_loss / max(1, num_groups)
		avg_align_loss = avg_align_loss / max(1, num_groups)
		avg_sup_loss = avg_sup_loss / max(1, num_groups)
		sim = sim_accum / max(1, num_groups)
		pred_adj = (sim > self.edge_threshold).float()
		pred_adj.fill_diagonal_(0)
		# 将下三角部分（i > j）置为0
		mask_lower = torch.tril(torch.ones_like(pred_adj), diagonal=-1).bool()
		pred_adj[mask_lower] = 0
		true_adj = true_adj.to(pred_adj.device)
		acc = self._calc_accuracy(pred_adj, true_adj)
		f1 = self._calc_f1(pred_adj, true_adj)
		auc = self._calc_auc(sim, true_adj)
		accs.append(acc)
		f1s.append(f1)
		aucs.append(auc)
		return {"accuracy": sum(accs) / max(1, len(accs)),"f1": sum(f1s) / max(1, len(f1s)),"auc": sum(aucs) / max(1, len(aucs)),"sim": sim}
		
	@torch.no_grad()
	def evaluate(self, g):

		self.model.eval()
		accs: List[float] = []
		f1s: List[float] = []
		aucs: List[float] = []
		sim_accum = None
		node_outputs_all: List[List[str]] = g['node_outputs']
		true_adj = g['adjacency_matrix'].to(self.device)

		node_outputs_batch = list(zip(*node_outputs_all))
		

		first_perturb_group = node_outputs_batch[0] if node_outputs_batch else []
		node_outputs_group = [[text] for text in first_perturb_group]
		task_for_llm = g["task"][0] if isinstance(g["task"], list) else g["task"]
		pair_log = self.llm_predict_edges_global_llm(
				task=task_for_llm,
				id_list=[str(i) for i in range(len(node_outputs_all))], 
				node_texts=node_outputs_group,
				llm_max_new_tokens=1024,
				llm_temperature=0.01,
				llm_do_sample=False,
				device=str(self.device),
			)
		num_nodes = len(node_outputs_all)
		pair_log_adj = torch.zeros((num_nodes, num_nodes), device=self.device)
		for edge in pair_log:
			if len(edge) == 2:
				source_id, target_id = int(edge[0]), int(edge[1])
				if 0 <= source_id < num_nodes and 0 <= target_id < num_nodes:
					pair_log_adj[source_id, target_id] = 1.0
		pair_log_adj.fill_diagonal_(0)
		node_outputs = [list(x) for x in zip(*node_outputs_all)]
		outputs = self.model(node_outputs,pair_log,training=False)
		sim = outputs['sim']
		pred_adj = (sim > self.edge_threshold).float()
		pred_adj.fill_diagonal_(0)
		mask_lower = torch.tril(torch.ones_like(pred_adj), diagonal=-1).bool()
		pred_adj[mask_lower] = 0
		true_adj = true_adj.to(pred_adj.device)
		acc = self._calc_accuracy(pred_adj, true_adj)
		f1 = self._calc_f1(pred_adj, true_adj)
		auc = self._calc_auc(sim, true_adj)
		accs.append(acc)
		f1s.append(f1)
		aucs.append(auc)
		return {
			"accuracy": sum(accs) / max(1, len(accs)),
			"f1": sum(f1s) / max(1, len(f1s)),
			"auc": sum(aucs) / max(1, len(aucs)),
		}



